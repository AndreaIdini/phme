---
title: Gli Androidi Sognano? (Pecore Elettriche)
author: Aaron Allegra
type: post
date: 2010-10-25T10:49:39+00:00
url: /blog/2010/10/25/gli-androidi-sognano-pecore-elettriche/
categories:
  - Ph.ysics
tags:
  - Filosofia
  - Informatica
  - Mente

---
<figure style="width: 347px" class="wp-caption alignleft"><img src="http://upload.wikimedia.org/wikipedia/commons/1/17/ArtificialFictionBrain.png" alt="" width="347" height="314" /><figcaption class="wp-caption-text">da wikipedia</figcaption></figure> 

Che differenza c'è tra un essere umano e un normale Personal Computer?  
Forse poca. O almeno così pensano i numerosi scienziati dietro i progetti di ricerca sullo sviluppo di una vera e propria Intelligenza Artificiale all'altezza dei suoi creatori - nonché, ovviamente, coloro che sono disposti a conceder loro copiosi finanziamenti.

Blade Runner, i racconti di Asimov, A.I di Spielberg, e persino serie tv ( pensiamo a Battlestar Galactica) : l'idea della macchina pensante ha colonizzato l'immaginario dell'uomo del secondo Novecento e dell'inizio del nuovo millennio; tanto da rendere facilmente accettabile per l'uomo comune l'idea che essa possa effettivamente “venire alla luce” in tempi relativamente brevi.  
Ma siamo sicuri che tanto ottimismo sia giustificato dallo stato dei fatti?  
**<!--more-->

  
Mente:Cervello=Programma:Hardware**  
La convinzione base della teoria dell' Intelligenza Artificiale Forte – quella che ritiene che i computer non _simulino_ soltanto delle menti, ma _siano_ delle menti – è figlia di un'idea che ha influenzato profondamente il corso della filosofia contemporanea.

Ad un certo punto della storia – con il progredire delle nostre conoscenze nel campo della fisica, della biologia, della psicologia e delle neuroscienze – ci si è chiesti: “Dato che la scienza ci mostra che viviamo in un mondo composto da atomi in relazione tra loro, è forse possibile spiegare anche i nostri stati mentali soggettivi, la nostra coscienza, in questi termini o in termini affini?”; il che, per la verità, non sarebbe affatto un'idea nuova.

Il materialismo non appartiene solo al ventesimo e ventunesimo secolo. Possiamo però dire che le vittorie delle scienze fisiche – le quali nella loro visione classica indagano, per definizione, solo ciò che è misurabile e quindi fisico e che giorno dopo giorno ci danno una visione del mondo sempre più esaustiva – ha fatto del materialismo – prima una mera ipotesi – quasi una convinzione di fondo di buona parte degli scienziati e di ampie fette della gente comune.

Inoltre, se considerassimo la coscienza qualcosa di non-materiale, come potremmo spiegare il fatto che essa causi eventi fisici (cosa che accade ogni volta che la nostra volontà causa determinate azioni), dato che sappiamo che uno stato fisico è causato interamente e solamente da un altro stato fisico (ad esso precedente, secondo la versione fisica classica: è l'idea di un mondo _causalmente chiuso_)? Conseguenza: i nostri stati mentali soggettivi devono essere spiegabili in termini di, devono essere interamente spiegabili in termini di, coincidere con, non essere altro che un certo tipo di eventi, o entità, fisiche o spiegabili in termini compatibili con quelli della fisica.  
Perchè non _ridurre_, dunque, un dolore o una credenza soggettiva a un certo tipo di interazione tra neuroni? (E' il materialismo “classico”)

O meglio ancora: dato che ad un dolore e ad una credenza soggettiva possono corrispondere più occorrenze di interazioni tra neuroni, cos'è che accomuna questi diversi _modi di realizzabilità_ facendo sì che essi corrispondano tutti ad un certo tipo di stato mentale? (Dovremmo dire meglio “siano coincidenti con”: ma ovviamente, se la ponessimo in questi termini la teoria mostrerebbe già da subito la sua intrinseca debolezza – è molto difficile che differenti modi di realizzabilità siano tutti coincidenti con un medesimo stato mentale, se abbiamo detto che esso dovrebbe essere _nient'altro che_ la sua base neuronale.)

La risposta che qui ci interessa è questa: essi sono tutti accomunati dalla funzione causale che svolgono nell'organismo e nei suoi rapporti con l'esterno. Questa teoria è chiamata _funzionalismo_. Una credenza è ciò che è causato da determinate percezioni (anch'esse definibili in termini di relazioni causali e assimilabili a certe funzioni), e che causa determinati comportamenti dell'organismo. A questo punto evidentemente  la base materiale e neuronale – pur sempre data tacitamente per assodata – degli stati mentali assume un'importanza relativa. Non importa sapere quale sia l'esatta occorrenza al livello dell'interazione tra neuroni corrispondente ad un certo dolore, dato che tutto ciò che conta per definire un dolore è la funzione che esso svolge nell'organismo. E ancora meno conta l'aspetto soggettivo, qualitativo del dolore: _cosa si prova esattamente a sperimentare un certo dolore_ , quello che per il comune mortale è anzi _l'aspetto determinante_ per definire un certo stato mentale, qui cade completamente al di fuori della definizione filosofica di questo.

Un dolore, una credenza, non è nient'altro che una funzione.  
E una funzione può essere svolta da qualsiasi cosa adatta a questo compito: così come non importa il materiale di cui è costituito un orologio, che _<<può essere fatto da ingranaggi e rotelle, può essere costituito da una clessidra a sabbia, o da oscillatori al quarzo>>_ (1)    per definire cosa esso sia, ma solo la funzione che esso svolge – ossia misurare il tempo – allo stesso modo uno stato mentale definito solo nei termini della sua funzione potrebbe essere realizzato da qualsiasi cosa adatta a questo compito.  
Dunque, anche da un sostrato materiale diverso dal cervello e da interazioni diverse rispetto a quelle tra neuroni. Perchè un computer, che può realizzare computazioni e quelle che sembrano operazioni logiche, non potrebbe provare stati mentali? Perchè non può essere altrettanto buono che il nostro cervello? Anzi, non è probabile che anche per noi esseri umani **la mente stia al cervello come il programma sta all'hardware**? Il cervello non è altro che un calcolatore biologico. La mente, la coscienza, è il programma che esso usa.

E questo è quanto.

**Ciò che un computer non potrà mai avere (e, anche se lo avesse, noi non lo sapremmo)**

La debolezza intrinseca di questa teoria deve esservi già almeno in parte chiara. Non ho potuto fare a meno di notare brevemente quello che questo modo di definire uno stato mentale, con tutto ciò che ne consegue per l'idea della I.A. Forte, lascia fuori ciò che istintivamente, e non a torto, noi riteniamo come l'essenza fondamentale di ogni stato psichico. Ossia, il suo aspetto soggettivo e qualitativo.  
Nei termini usati dal funzionalismo, l'accento viene posto non su _cosa provi,_ ma soprattutto su  come reagisci in termini causali e su cosa ha causato la tua reazione. In poche parole, forse non viene sottolineata adeguatamente la possibile differenza tra una simulazione ben riuscita, tra un automatismo perfetto, e un _atto cosciente_. Noi, in quanto esseri umani, non possiamo non sapere che invece una differenza c'è. Ed è abissale. Forse questa differenza potrà non essere evidente ad un osservatore esterno – ma per _la nostra stessa esistenza_ questa differenza **è la differenza fondamentale**. Il punto è che ciascuno di noi percepisce intuitivamente (e con il massimo dell'evidenza, per dirla con Cartesio) che la sua vita psichica non è data semplicemente da una concatenazioni di stimoli e reazioni, ma anche e soprattutto **dalla coscienza di questi stimoli e delle seguenti reazioni** (anche se, evidentemente, questa vita mentale si basa su ciò che **non** percepiamo: i processi automatici e, forse, l'inconscio). Noi esseri umani non abbiamo solo una buona struttura di regole logiche. Siamo anche, e soprattutto, capaci di dare a queste regole un significato. Abbiamo la sintassi – l'insieme di regole che guidano il nostro comportamento, definibile in termini di stimolo e risposta e di relazioni causali – e, **anche e soprattutto**, abbiamo la semantica. La capacità di attribuire a queste regole un significato. La capacità di renderci conto di quel che accade. Possiamo passare dalla constatazione che siamo in grado di dare ad un calcolatore elettronico un insieme di regole funzionali – _la sintassi_ – alla convinzione che potremo fornire ad essi _la semantica_, **la comprensione** del significato dei simboli con cui lavorano? Al momento, no. Non abbiamo la più pallida idea di come all'interno dei nostri cervelli si passi dall'interazione neuronale e dalla struttura sintattica alla coscienza ed alla sfera dei significati e della consapevolezza. Non possediamo alcuna evidenza del fatto che questo passaggio sia ripetibile per mezzo della nostra tecnica; e ciò, semplicemente, perchè non sappiamo _cosa sia_ e come si ottenga la coscienza. Tanto che di fronte a questo anche la teoria classica del materialismo, la convinzione di fondo di buona parte degli uomini di cultura del nostro tempo, sembra vacillare, o almeno aver bisogno di un'ulteriore riformulazione.

_Un argomento esplicativo contro la I.A. Forte: la Stanza Cinese (di John Searle).  
Da Wikipedia (2):_

> I sostenitori dell' intelligenza artificiale forte sostengono che un computer opportunamente programmato non sia solo la simulazione o un modello della mente, ma che esso possa essere una mente. Esso cioè capisce, ha condizioni conoscitive e può pensare. L'argomento di Searle (o meglio, l'esperimento mentale) si oppone a questa posizione. L'argomentazione della stanza cinese è la seguente:  
> Si supponga che, nel futuro, si possa costruire un computer che si comporti come se capisse il cinese. In altre parole, il computer prenderebbe dei simboli cinesi in ingresso, consulterebbe una grande tabella che gli consenta di produrre altri simboli cinesi in uscita. Si supponga che il comportamento di questo computer sia così convincente da poter facilmente superare il test di Turing. In altre parole, il computer possa convincere un uomo che parla correttamente cinese (per esempio un cinese) di parlare con un altro uomo che parla correttamente cinese, mentre in realtà sta parlando con un calcolatore. A tutte le domande dell'umano il computer risponderebbe appropriatamente, in modo che l'umano si convinca di parlare con un altro umano che parla correttamente cinese. I sostenitori dell'intelligenza artificiale forte concludono che il computer capisce la lingua cinese, come farebbe una persona, in quanto non c'è nessuna differenza tra il comportamento della macchina e di un uomo che conosce il cinese.
> 
> **Ora, Searle chiede di supporre che lui si sieda all'interno del calcolatore. In altre parole, egli si immagina in una piccola stanza (la stanza cinese) dalla quale riceva dei simboli cinesi, e una tabella che gli consenta di produrre dei simboli cinesi in uscita in modo identico a quanto faceva il programma seguìto dal calcolatore. Searle fa notare che egli non capisce i simboli cinesi. Quindi la sua mancanza di comprensione dimostra che il calcolatore non può comprendere il cinese, poiché esso è nella sua stessa situazione. Il calcolatore è un semplice manipolatore di simboli, esattamente come lo è lui nella stanza cinese - e quindi i calcolatori non capiscono quello che stanno dicendo tanto quanto lui.**

Del resto, anche se un computer potesse ottenere – per ragioni che noi oggi non comprendiamo  che non sapremmo spiegare sulla base delle nostre attuali conoscenze – l'accesso alla sfera dei significati e della semantica, come potremmo saperlo? Come potremmo distinguere una buona simulazione automatica da una reale comprensione di ciò che si fa? Semplice: non potremmo. Qualsiasi criterio funzionale o comportamentistico per affrontare tale questione è sempre fallito e continuerà ad essere una strada impraticabile. Semplicemente perchè a rigore non si può mai dedurre la presenza di _una coscienza_ dai comportamenti esterni e nemmeno da relazioni causali, a maggior ragione per ciò che riguarda simulazioni che _noi_ abbiamo creato allo scopo di emulare i comportamenti umani. Tutto ciò che possiamo dire adesso è che non abbiamo alcuna evidenza scientifica per supporlo. Non ne sappiamo affatto abbastanza.

**Una seconda considerazione contro l' I.A. Forte: “Che cos'è un'informazione”?**

Un secondo assunto accettato tacitamente dai sostenitori dell'I.A. Forte è che si possa lecitamente parlare di “dati”, “informazioni”, “memoria” per ciò che si riferisce ai processi di un P.C, alle operazioni di un calcolatore o alle sue componenti.

In che senso l'input che il mio notebook sta elaborando sarebbe un'informazione? In che senso ciò che lascio scritto sul mio disco fisso farebbe parte della sua memoria? L'uso di tali termini giova non poco alla convinzione che un computer manipoli _effettivamente_ dei contenuti che noi percepiamo come informazioni e, quindi, appartenenti alla sfera dei significati.  
Ma, a rigor di logica, tutto ciò che il computer ci restituisce e che viene interpretato da noi come _contenuto_ e _informazione_, può essere tale solo in relazione ad un osservatore in grado di attribuire a ciò che appare sul monitor, e a certi risultati dei processi di calcolo, uno specifico significato.

Ciò che il computer ci restituisce non è informazione di per sé – semplicemente perchè _non esiste nulla che sia informazione di per sé_, e tutto può essere o non essere informazione solo a seconda dell'interpretazione che ne dà un osservatore cosciente e in possesso della semantica – della sfera dei significati. La nostra voce, il suono di un violino, un gesto, un segno grafico su un quaderno – tutte queste cose _di per sé_ sono onde sonore, tracce e spostamenti di atomi. Siamo _noi in quanto esseri coscienti_ a decodificarli come altrettanti messaggi. Citando nuovamente John Searle:

> <<_Quando digito “2+2=” sulla mia calcolatrice tascabile e la mia calcolatrice tascabile mi dà “4”, essa non sa nulla di computazione, aritmetica, o simboli, perchè non sa nulla di nulla. Intrinsecamente, è un circuito elettronico complesso che usiamo per fare calcoli. Le transizioni di stato elettrico sono intrinseche alla macchina, ma il calcolo è nell'occhio dell'osservatore._>>(3)

Dunque concludendo a rigore non c'è nulla che possa spingerci a credere, ad oggi, che alla crescente complessità e velocità di calcolo (e per il concetto di calcolo vedasi sopra) dei nostri PC possa un giorno corrispondere una reale comprensione dei significati quale noi esseri umani abbiamo. Essi potranno tutt'al più fornire _una buona simulazione della mente_; nel senso che saremo noi a dare ai processi elettrici della macchina e ai loro risultati i significati che vorremo, che la macchina riuscirà forse ad elaborare – grazie alle regole “sintattiche” che _noi_ le daremo – e a manipolare con precisione e velocità sempre crescente. Forse la Singolarità Tecnologica dovrà attendere – il che, tutto sommato, potrebbe essere una buona notizia.

**Fonti**  
(1)John Searle, “La Mente”, pagina 58  
(2)<http://it.wikipedia.org/wiki/Stanza_cinese>  
(3)John Searle, “La Mente”, pagina 83

## Comments

### Comment by Andrea on 2012-01-10 22:33:36 +0000
Prova